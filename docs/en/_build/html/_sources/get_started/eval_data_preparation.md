# ðŸ—‚ï¸ Evaluation Data Preparation

> [COCO](https://cocodataset.org/) images are used in VQAv2, OK-VQA, RefCOCO, POPE, and so on. Make sure you have already downloaded COCO images before evaluating on these benchmarks.

## Image Captioning

### COCO

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/coco && cd data/coco

# Step 2: Download and unzip image files
wget http://images.cocodataset.org/zips/train2014.zip && unzip train2014.zip
wget http://images.cocodataset.org/zips/val2014.zip && unzip val2014.zip
wget http://images.cocodataset.org/zips/test2015.zip && unzip test2015.zip

# Step 3: Download and place the annotation files
mkdir -p annotations && cd annotations/
wget https://github.com/OpenGVLab/InternVL/releases/download/data/coco_karpathy_test.json
wget https://github.com/OpenGVLab/InternVL/releases/download/data/coco_karpathy_test_gt.json

cd ../../..
```

After preparation is complete, the directory structure is:

```
data/coco
â”œâ”€â”€ annotations
â”‚   â”œâ”€â”€ coco_karpathy_test.json
â”‚   â””â”€â”€ coco_karpathy_test_gt.json
â”œâ”€â”€ train2014
â”œâ”€â”€ val2014
â””â”€â”€ test2015
```

### Flickr30K

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/flickr30k && cd data/flickr30k

# Step 2: Download and unzip image files
# Download images from https://bryanplummer.com/Flickr30kEntities/

# Step 3: Download and place the annotation files
# Karpathy split annotations can be downloaded from the following link:
wget https://github.com/mehdidc/retrieval_annotations/releases/download/1.0.0/flickr30k_test_karpathy.txt
# This file is provided by the clip-benchmark repository.
# We convert this txt file to json format, download the converted file:
wget https://github.com/OpenGVLab/InternVL/releases/download/data/flickr30k_test_karpathy.json

cd ../..
```

After preparation is complete, the directory structure is:

```
data/flickr30k
â”œâ”€â”€ Images
â”œâ”€â”€ flickr30k_test_karpathy.txt
â””â”€â”€ flickr30k_test_karpathy.json
```

### NoCaps

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/nocaps && cd data/nocaps

# Step 2: Download and unzip image files
# Download images from https://nocaps.org/download

# Step 3: Download and place the annotation files
# Original annotations can be downloaded from https://nocaps.s3.amazonaws.com/nocaps_val_4500_captions.json
wget https://nocaps.s3.amazonaws.com/nocaps_val_4500_captions.json

cd ../..
```

After preparation is complete, the directory structure is:

```
data/nocaps
â”œâ”€â”€ images
â””â”€â”€ nocaps_val_4500_captions.json
```

## Reasoning & Mathematics

### MMMU

> âš ï¸ Note: While our codebase can run the benchmark, we recommend using [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) for testing this benchmark if you aim to align results with our technical report.

The evaluation script will automatically download the MMMU dataset from HuggingFace.

### MMMU-Pro

The evaluation script will automatically download the MMMU-Pro dataset from HuggingFace.

### MathVista

> âš ï¸ Note: While our codebase can run the benchmark, we recommend using [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) for testing this benchmark if you aim to align results with our technical report.

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/MathVista && cd data/MathVista

# Step 2: Download the annotation
wget https://huggingface.co/datasets/AI4Math/MathVista/raw/main/annot_testmini.json

cd ../..
```

After preparation is complete, the directory structure is:

```
MathVista
â””â”€â”€ annot_testmini.json
```

## OCR & Chart & Document

### AI2D

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/ai2diagram && cd data/ai2diagram

# Step 2: Download converted files
wget https://huggingface.co/OpenGVLab/InternVL/raw/main/ai2d_test_vlmevalkit.jsonl -O test_vlmevalkit.jsonl
wget https://huggingface.co/OpenGVLab/InternVL/resolve/main/AI2D_TEST.zip && unzip AI2D_TEST.zip

# Step 3: Download images from Google Drive (optional, provided by InternLM-XComposer)
# https://drive.google.com/file/d/1dqqa3MnrxMXaU_K9JA6C83je32ibwdOY/view?usp=sharing
# images should be placed in `data/ai2diagram/ai2d/abc_images` and `data/ai2diagram/ai2d/images`

cd ../..
```

After preparation is complete, the directory structure is:

```
data/ai2diagram
 â”œâ”€â”€ test_vlmevalkit.jsonl
 â”œâ”€â”€ ai2d # (optional)
 â”‚    â”œâ”€â”€ abc_images
 â”‚    â””â”€â”€ images
 â””â”€â”€ AI2D_TEST
```

### ChartQA

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/chartqa && cd data/chartqa

# Step 2: Download images from
# https://drive.google.com/file/d/1Lm_w6zeET1Hyl_9ks6w5nEsgpoyPHalV/view

# Step 3: Download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/chartqa/train_human.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/chartqa/train_augmented.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/chartqa/test_human.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/chartqa/test_augmented.jsonl

cd ../..
```

### TextVQA

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/textvqa && cd data/textvqa

# Step 2: Download images
wget https://dl.fbaipublicfiles.com/textvqa/images/train_val_images.zip && unzip train_val_images.zip

# Step 3: Download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_train_annotations.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_train_questions.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_val_annotations.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/textvqa/textvqa_val_questions.json
wget https://huggingface.co/OpenGVLab/InternVL/raw/main/textvqa_val.jsonl
wget https://huggingface.co/OpenGVLab/InternVL/raw/main/textvqa_val_llava.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/textvqa
â”œâ”€â”€ TextVQA_Rosetta_OCR_v0.2_test.json
â”œâ”€â”€ TextVQA_Rosetta_OCR_v0.2_train.json
â”œâ”€â”€ TextVQA_Rosetta_OCR_v0.2_val.json
â”œâ”€â”€ textvqa_train_annotations.json
â”œâ”€â”€ textvqa_train.jsonl
â”œâ”€â”€ textvqa_train_questions.json
â”œâ”€â”€ textvqa_val_annotations.json
â”œâ”€â”€ textvqa_val.jsonl
â”œâ”€â”€ textvqa_val_llava.jsonl
â”œâ”€â”€ textvqa_val_questions.json
â””â”€â”€ train_images
```

After preparation is complete, the directory structure is:

```
data/chartqa
 â”œâ”€â”€ ChartQA Dataset
 â”‚    â”œâ”€â”€ test
 â”‚    â”œâ”€â”€ train
 â”‚    â””â”€â”€ val
 â”œâ”€â”€ test_augmented.jsonl
 â”œâ”€â”€ test_human.jsonl
 â”œâ”€â”€ train_augmented.jsonl
 â””â”€â”€ train_human.jsonl
```

### DocVQA

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/docvqa && cd data/docvqa

# Step 2: Download images and annotations
wget https://datasets.cvc.uab.es/rrc/DocVQA/train.tar.gz --no-check-certificate # (optional)
wget https://datasets.cvc.uab.es/rrc/DocVQA/val.tar.gz --no-check-certificate
wget https://datasets.cvc.uab.es/rrc/DocVQA/test.tar.gz --no-check-certificate

# Step 3: Unzip files
tar -zxvf train.tar.gz
tar -zxvf val.tar.gz
tar -zxvf test.tar.gz

# Step 4: Download converted jsonl files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/docvqa/train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/docvqa/val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/docvqa/test.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/docvqa
â”œâ”€â”€ test
â”œâ”€â”€ test.jsonl
â”œâ”€â”€ train
â”œâ”€â”€ train.jsonl
â”œâ”€â”€ val
â””â”€â”€ val.jsonl
```

### InfoVQA

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/infographicsvqa && cd data/infographicsvqa

# Step 2: Download images and annotations from https://rrc.cvc.uab.es/?ch=17&com=downloads
# infographicsVQA_test_v1.0.json, infographicsVQA_val_v1.0_withQT.json, infographicVQA_train_v1.0.json

# Step 3: Download converted files
wget https://huggingface.co/OpenGVLab/InternVL/raw/main/infographicsvqa_val.jsonl -O val.jsonl
wget https://huggingface.co/OpenGVLab/InternVL/raw/main/infographicsvqa_test.jsonl -O test.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/infographicsvqa
â”œâ”€â”€ infographicsvqa_images
â”œâ”€â”€ infographicsVQA_test_v1.0.json
â”œâ”€â”€ infographicsVQA_val_v1.0_withQT.json
â”œâ”€â”€ infographicVQA_train_v1.0.json
â”œâ”€â”€ test.jsonl
â””â”€â”€ val.jsonl
```

### OCRVQA

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/ocrvqa && cd data/ocrvqa

# Step 2: Download images by following instructions at 
# https://ocr-vqa.github.io/kvqa_ProjectFiles/README.txt

# Step 3: Download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/ocrvqa/ocrvqa_train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/ocrvqa/ocrvqa_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/ocrvqa/ocrvqa_test.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/ocrvqa
â”œâ”€â”€ images
â”œâ”€â”€ ocrvqa_test.jsonl
â”œâ”€â”€ ocrvqa_train.jsonl
â””â”€â”€ ocrvqa_val.jsonl
```

## Multi-Image

### Mantis-Eval

The evaluation script will automatically download the Mantis Eval dataset from HuggingFace.

### MMIU

Follow the instructions below to prepare the data:

```shell
# Step 1: Create the data directory
mkdir -p data/mmiu && cd data/mmiu

# Step 2: Download images
wget https://huggingface.co/MMIUBenchmark/MMIU/resolve/main/2D-spatial.zip
wget https://huggingface.co/MMIUBenchmark/MMIU/resolve/main/3D-spatial.zip
unzip 2D-spatial.zip
unzip 3D-spatial.zip

cd ../..
```

After preparation is complete, the directory structure is:

```shell
data/mmiu
 â”œâ”€â”€ 2D-spatial
 â””â”€â”€ 3D-spatial
```

### MIRB

Follow the instructions below to prepare the data:

```shell
# Step 1: Download annotation files
cd data/
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/datasets/VLLMs/MIRB

# Step 2: Download and unzip the image files
cd MIRB/ && rm -rf images.zip
wget https://huggingface.co/datasets/VLLMs/MIRB/resolve/main/images.zip
unzip images.zip

cd ../../
```

After preparation is complete, the directory structure is:

```shell
data/MIRB
â”œâ”€â”€ images
â”œâ”€â”€ ...
â”œâ”€â”€ visual_chain.json
â””â”€â”€ visual_chain_concat.json
```

## Comprehensive

### MME

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/mme && cd data/mme

# Step 2: Download MME_Benchmark_release_version.zip
wget https://huggingface.co/OpenGVLab/InternVL/resolve/main/MME_Benchmark_release_version.zip
unzip MME_Benchmark_release_version.zip

cd ../..
```

After preparation is complete, the directory structure is:

```
data/mme
 â””â”€â”€ MME_Benchmark_release_version
```

### MMBench & CCBench

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/mmbench && cd data/mmbench

# Step 2: Download csv files
wget http://opencompass.openxlab.space/utils/MMBench/CCBench_legacy.tsv
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_dev_20230712.tsv
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_dev_cn_20231003.tsv
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_dev_en_20231003.tsv
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_test_cn_20231003.tsv
wget https://download.openmmlab.com/mmclassification/datasets/mmbench/mmbench_test_en_20231003.tsv

cd ../..
```

After preparation is complete, the directory structure is:

```

data/mmbench
 â”œâ”€â”€ CCBench_legacy.tsv
 â”œâ”€â”€ mmbench_dev_20230712.tsv
 â”œâ”€â”€ mmbench_dev_cn_20231003.tsv
 â”œâ”€â”€ mmbench_dev_en_20231003.tsv
 â”œâ”€â”€ mmbench_test_cn_20231003.tsv
 â””â”€â”€ mmbench_test_en_20231003.tsv
```

### MMVet

> âš ï¸ Note: While our codebase can run the benchmark, we recommend using [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) for testing this benchmark if you aim to align results with our technical report.

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/mm-vet && cd data/mm-vet

# Step 2: Download the dataset
wget https://github.com/yuweihao/MM-Vet/releases/download/v1/mm-vet.zip
unzip mm-vet.zip
wget https://huggingface.co/OpenGVLab/InternVL/raw/main/llava-mm-vet.jsonl
cd ../..
```

After preparation is complete, the directory structure is:

```
data/mm-vet
 â”œâ”€â”€ images
 â””â”€â”€ llava-mm-vet.jsonl
```

### MMVet v2

Follow the instructions below to prepare the data:

```shell
# Step 1: Create the data directory
mkdir -p data/mm-vet-v2 && cd data/mm-vet-v2

# Step 2: Download the dataset
wget https://github.com/yuweihao/MM-Vet/releases/download/v2/mm-vet-v2.zip
unzip mm-vet-v2.zip

cd ../..
```

After preparation is complete, the directory structure is:

```shell
data/mm-vet-v2
 â”œâ”€â”€ images
 â””â”€â”€ mm-vet-v2.json
```

## Hallucination

### MMHal-Bench

Follow the instructions below to prepare the data:

```shell
# Step 1: Create the data directory
mkdir -p data/mm-halbench && cd data/mm-halbench

# Step 2: Download the `mmhal-bench_with_image.jsonl` file
# This file is provided by RLAIF-V
# See here: https://github.com/RLHF-V/RLAIF-V/blob/main/README.md#mmhal-bench
wget https://huggingface.co/OpenGVLab/InternVL/resolve/main/mmhal-bench_with_image.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```shell
data/mm-halbench
 â””â”€â”€ mmhal-bench_with_image.jsonl
```

### POPE

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/pope && cd data/pope

# Step 2: Make sure you have downloaded COCO images
ln -s ../coco/val2014 ./
wget https://github.com/OpenGVLab/InternVL/releases/download/data/llava_pope_test.jsonl

# Step 3: Download `coco` from POPE
mkdir -p coco && cd coco
wget https://github.com/AoiDragon/POPE/raw/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco/coco_pope_adversarial.json
wget https://github.com/AoiDragon/POPE/raw/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco/coco_pope_popular.json
wget https://github.com/AoiDragon/POPE/raw/e3e39262c85a6a83f26cf5094022a782cb0df58d/output/coco/coco_pope_random.json
cd ../../..
```

After preparation is complete, the directory structure is:

```
data/pope
â”œâ”€â”€ coco
â”‚   â”œâ”€â”€ coco_pope_adversarial.json
â”‚   â”œâ”€â”€ coco_pope_popular.json
â”‚   â””â”€â”€ coco_pope_random.json
â”œâ”€â”€ llava_pope_test.jsonl
â””â”€â”€ val2014
```

## Visual Grounding

### RefCOCO Series

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/refcoco && cd data/refcoco

# Step 2: Download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco/refcoco_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco/refcoco_testA.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco/refcoco_testB.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco%2B/refcoco%2B_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco%2B/refcoco%2B_testA.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcoco%2B/refcoco%2B_testB.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcocog/refcocog_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/refcocog/refcocog_test.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/refcoco
â”œâ”€â”€ refcocog_test.jsonl
â”œâ”€â”€ refcocog_val.jsonl
â”œâ”€â”€ refcoco_testA.jsonl
â”œâ”€â”€ refcoco+_testA.jsonl
â”œâ”€â”€ refcoco_testB.jsonl
â”œâ”€â”€ refcoco+_testB.jsonl
â”œâ”€â”€ refcoco_val.jsonl
â””â”€â”€ refcoco+_val.jsonl
```

## Video

### MVBench

Follow the instructions below to prepare the data:

```shell
# Step 1: Download the dataset
cd data/
huggingface-cli download --repo-type dataset --resume-download OpenGVLab/MVBench --local-dir MVBench --local-dir-use-symlinks False

# Step 2: Unzip videos
cd MVBench/video/
for file in *.zip; do unzip "$file" -d "${file%.*}"; done
cd ../../..
```

After preparation is complete, the directory structure is:

```shell
data/MVBench
â”œâ”€â”€ json
â”‚   â”œâ”€â”€ action_antonym.json
â”‚   â”œâ”€â”€ action_count.json
â”‚   â”œâ”€â”€ action_localization.json
â”‚   â”œâ”€â”€ action_prediction.json
â”‚   â”œâ”€â”€ action_sequence.json
â”‚   â”œâ”€â”€ character_order.json
â”‚   â”œâ”€â”€ counterfactual_inference.json
â”‚   â”œâ”€â”€ egocentric_navigation.json
â”‚   â”œâ”€â”€ episodic_reasoning.json
â”‚   â”œâ”€â”€ fine_grained_action.json
â”‚   â”œâ”€â”€ fine_grained_pose.json
â”‚   â”œâ”€â”€ moving_attribute.json
â”‚   â”œâ”€â”€ moving_count.json
â”‚   â”œâ”€â”€ moving_direction.json
â”‚   â”œâ”€â”€ object_existence.json
â”‚   â”œâ”€â”€ object_interaction.json
â”‚   â”œâ”€â”€ object_shuffle.json
â”‚   â”œâ”€â”€ scene_transition.json
â”‚   â”œâ”€â”€ state_change.json
â”‚   â””â”€â”€ unexpected_action.json
â”œâ”€â”€ README.md
â””â”€â”€ video
    â”œâ”€â”€ clevrer
    â”œâ”€â”€ FunQA_test
    â”œâ”€â”€ Moments_in_Time_Raw
    â”œâ”€â”€ nturgbd
    â”œâ”€â”€ perception
    â”œâ”€â”€ scene_qa
    â”œâ”€â”€ ssv2_video
    â”œâ”€â”€ sta
    â”œâ”€â”€ star
    â”œâ”€â”€ tvqa
    â””â”€â”€ vlnqa
```

## General VQA

### VQAv2

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/vqav2 && cd data/vqav2

# Step 2: Make sure you have downloaded COCO images
ln -s ../coco/train2014 ./
ln -s ../coco/val2014 ./
ln -s ../coco/test2015 ./

# Step 3: Download questions and annotations
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip && unzip v2_Annotations_Train_mscoco.zip
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip && unzip v2_Questions_Train_mscoco.zip
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip && unzip v2_Annotations_Val_mscoco.zip
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip && unzip v2_Questions_Val_mscoco.zip
wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Test_mscoco.zip && unzip v2_Questions_Test_mscoco.zip

# Step 4: Download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vqav2/vqav2_train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vqav2/vqav2_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vqav2/vqav2_testdev.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/vqav2
â”œâ”€â”€ train2014 -> ../coco/train2014
â”œâ”€â”€ val2014 -> ../coco/val2014
â”œâ”€â”€ test2015 -> ../coco/test2015
â”œâ”€â”€ v2_mscoco_train2014_annotations.json
â”œâ”€â”€ v2_mscoco_train2014_complementary_pairs.json
â”œâ”€â”€ v2_mscoco_val2014_annotations.json
â”œâ”€â”€ v2_OpenEnded_mscoco_test2015_questions.json
â”œâ”€â”€ v2_OpenEnded_mscoco_test-dev2015_questions.json
â”œâ”€â”€ v2_OpenEnded_mscoco_train2014_questions.json
â”œâ”€â”€ v2_OpenEnded_mscoco_val2014_questions.json
â”œâ”€â”€ vqav2_testdev.jsonl
â”œâ”€â”€ vqav2_train.jsonl
â””â”€â”€ vqav2_val.jsonl
```

### OKVQA

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/okvqa && cd data/okvqa

# Step 2: Make sure you have downloaded COCO images
ln -s ../coco/train2014 ./
ln -s ../coco/val2014 ./

# Step 3: Download annotations and questions
wget https://okvqa.allenai.org/static/data/mscoco_train2014_annotations.json.zip && unzip mscoco_train2014_annotations.json.zip
wget https://okvqa.allenai.org/static/data/OpenEnded_mscoco_train2014_questions.json.zip && unzip OpenEnded_mscoco_train2014_questions.json.zip
wget https://okvqa.allenai.org/static/data/mscoco_val2014_annotations.json.zip && unzip mscoco_val2014_annotations.json.zip
wget https://okvqa.allenai.org/static/data/OpenEnded_mscoco_val2014_questions.json.zip && unzip OpenEnded_mscoco_val2014_questions.json.zip

# Step 4: Download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/okvqa/okvqa_train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/okvqa/okvqa_val.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/okvqa
â”œâ”€â”€ mscoco_train2014_annotations.json
â”œâ”€â”€ mscoco_val2014_annotations.json
â”œâ”€â”€ okvqa_train.jsonl
â”œâ”€â”€ okvqa_val.jsonl
â”œâ”€â”€ OpenEnded_mscoco_train2014_questions.json
â”œâ”€â”€ OpenEnded_mscoco_val2014_questions.json
â”œâ”€â”€ test2014 -> ../coco/test2014
â””â”€â”€ val2014 -> ../coco/val2014
```

### VizWiz

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/vizwiz && cd data/vizwiz

# Step 2: Download images
wget https://vizwiz.cs.colorado.edu/VizWiz_final/images/train.zip && unzip train.zip
wget https://vizwiz.cs.colorado.edu/VizWiz_final/images/val.zip && unzip val.zip
wget https://vizwiz.cs.colorado.edu/VizWiz_final/images/test.zip && unzip test.zip

# Step 3: Download annotations
wget https://vizwiz.cs.colorado.edu/VizWiz_final/vqa_data/Annotations.zip && unzip Annotations.zip

# Step 4: Download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_train_annotations.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_train_questions.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_train.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_val_annotations.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_val_questions.json
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_val.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/vizwiz/vizwiz_test.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/vizwiz
â”œâ”€â”€ annotations
â”œâ”€â”€ test
â”œâ”€â”€ train
â”œâ”€â”€ val
â”œâ”€â”€ vizwiz_test.jsonl
â”œâ”€â”€ vizwiz_train_annotations.json
â”œâ”€â”€ vizwiz_train.jsonl
â”œâ”€â”€ vizwiz_train_questions.json
â”œâ”€â”€ vizwiz_val_annotations.json
â”œâ”€â”€ vizwiz_val.jsonl
â””â”€â”€ vizwiz_val_questions.json
```

### GQA

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/gqa && cd data/gqa

# Step 2: Download the official evaluation script
wget https://nlp.stanford.edu/data/gqa/eval.zip
unzip eval.zip

# Step 3: Download images
wget https://downloads.cs.stanford.edu/nlp/data/gqa/images.zip
unzip images.zip

# Step 4: Download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/gqa/testdev_balanced.jsonl
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/gqa/train_balanced.jsonl
wget https://github.com/OpenGVLab/InternVL/releases/download/data/llava_gqa_testdev_balanced_qwen_format.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/gqa
â”œâ”€â”€ challenge_all_questions.json
â”œâ”€â”€ challenge_balanced_questions.json
â”œâ”€â”€ eval.py
â”œâ”€â”€ images
â”œâ”€â”€ llava_gqa_testdev_balanced_qwen_format.jsonl
â”œâ”€â”€ readme.txt
â”œâ”€â”€ submission_all_questions.json
â”œâ”€â”€ test_all_questions.json
â”œâ”€â”€ test_balanced.jsonl
â”œâ”€â”€ test_balanced_questions.json
â”œâ”€â”€ testdev_all_questions.json
â”œâ”€â”€ testdev_balanced_all_questions.json
â”œâ”€â”€ testdev_balanced_predictions.json
â”œâ”€â”€ testdev_balanced_questions.json
â”œâ”€â”€ train_all_questions
â”œâ”€â”€ train_balanced.jsonl
â”œâ”€â”€ train_balanced_questions.json
â”œâ”€â”€ val_all_questions.json
â””â”€â”€ val_balanced_questions.json
```

### ScienceQA

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/scienceqa/images && cd data/scienceqa/images

# Step 2: Download images
wget https://scienceqa.s3.us-west-1.amazonaws.com/images/test.zip && unzip test.zip

cd ..

# Step 3: Download original questions
wget https://github.com/lupantech/ScienceQA/blob/main/data/scienceqa/problems.json

# Step 4: Download converted files
wget https://ofasys-wlcb.oss-cn-wulanchabu.aliyuncs.com/Qwen-VL/evaluation/scienceqa/scienceqa_test_img.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```
data/scienceqa
â”œâ”€â”€ images
â”œâ”€â”€ problems.json
â””â”€â”€ scienceqa_test_img.jsonl
```

### SEED-Image

Follow the instructions below to prepare the data:

```shell
# Step 1: Create the data directory
mkdir -p data/SEED && cd data/SEED

# Step 2: Download the dataset
wget https://huggingface.co/OpenGVLab/InternVL/resolve/main/SEED-Bench-image.zip
unzip SEED-Bench-image.zip
wget https://huggingface.co/OpenGVLab/InternVL/resolve/main/seed.jsonl

cd ../..
```

After preparation is complete, the directory structure is:

```shell
data/SEED
 â”œâ”€â”€ SEED-Bench-image
 â””â”€â”€ seed.jsonl
```

### MMVP

Follow the instructions below to prepare the data:

```shell
# Step 1: Download the dataset
cd data/
git clone https://huggingface.co/datasets/MMVP/MMVP
cd ../
```

After preparation is complete, the directory structure is:

```shell
data/MMVP
 â”œâ”€â”€ MMVP Images
 â”œâ”€â”€ Questions.csv
 â”œâ”€â”€ Questions.xlsx
 â””â”€â”€ README.md
```

### Tiny-LVLM-eHub

Follow the instructions below to prepare the dataï¼š

```bash
# Step 1: Create the data directory
mkdir -p data/tiny_lvlm && cd data/tiny_lvlm

# Step 2: Download the dataset
wget https://huggingface.co/OpenGVLab/InternVL/resolve/main/updated_datasets.zip
unzip updated_datasets.zip

cd ../..
```

After preparation is complete, the directory structure is:

```
data/tiny_lvlm
â””â”€â”€ updated_datasets
```

## Other Benchmarks

For other benchmarks mentioned in the [InternVL 2.5 technical report](https://arxiv.org/abs/2412.05271) but not listed here, please use [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) for evaluation.

<br>
<br>
